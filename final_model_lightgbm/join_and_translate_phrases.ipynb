{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_lambda_replace_characters(df, column_name, new_column, dictionary):\n",
    "    df[new_column] = df[column_name].apply(lambda x: replace_words(x, dictionary))       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(strtext, dictionary):\n",
    "    pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in dictionary.keys()) + r')(?!\\w)')\n",
    "    result = pattern.sub(lambda x: dictionary[x.group()], strtext)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../replacement_dictionary_ordered.json\") as json_file:\n",
    "    replacement_dictionary_2 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['TX', 'KS', 'AZ', 'TN', 'FL', 'GA', 'OK', 'UT', 'ME', 'CA', 'MS',\n",
    "       'WA', 'OH', 'MA', 'AL', 'NC', 'CO', 'MO', 'KY', 'NE', 'NY',\n",
    "       'VA', 'MD', 'HI', 'WV', 'MN', 'AK', 'IN', 'PA', 'SC', 'LA', 'NJ',\n",
    "       'RI', 'WY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = states[0]\n",
    "noun_file = 'new_avm_df__full_data_state_'+state+'_noun_sentences.fea'\n",
    "noun_sentence_df_T_ri_filtered = pd.read_feather(noun_file)\n",
    "\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered[noun_sentence_df_T_ri_filtered['count'] >\\\n",
    "                                                                noun_sentence_df_T_ri_filtered['count'].max()*0.005]\n",
    "\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence', 'count',\n",
    "                                                                      'AVM_Error_Indicators']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noun_sentence_df_T_ri_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'AVM_Error_Indicators'\n",
    "nelements = (len(noun_sentence_df_T_ri_filtered[column].values[0]))\n",
    "list_tmp = [str(i) for i in range(nelements)] \n",
    "column_names = [column + '_' + str(i) for i in list_tmp] \n",
    "print(column_names)\n",
    "split_df = pd.DataFrame(noun_sentence_df_T_ri_filtered[column].tolist(), columns=column_names, \n",
    "                        index=noun_sentence_df_T_ri_filtered.index)\n",
    "noun_sentence_df_T_ri_filtered = pd.concat([noun_sentence_df_T_ri_filtered, split_df], axis=1)\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.drop(columns = [column])\n",
    "\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence','count',\n",
    "                                                                       'AVM_Error_Indicators_0','AVM_Error_Indicators_1']]\n",
    "noun_sentence_df_T_ri_filtered = df_lambda_replace_characters(noun_sentence_df_T_ri_filtered,  'noun_sentence', \n",
    "                                                   'noun_sentence_corrections', replacement_dictionary_2)\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence_corrections','count',\n",
    "                                                                       'AVM_Error_Indicators_0','AVM_Error_Indicators_1']]\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.add_suffix('_'+state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full = noun_sentence_df_T_ri_filtered.copy(deep=True).\\\n",
    "                                rename(columns={'noun_sentence_corrections_'+state:'noun_sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full = noun_sentence_df_T_ri_filtered_full.drop_duplicates(subset='noun_sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full = noun_sentence_df_T_ri_filtered_full.set_index('noun_sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.sort_values('count_TX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.loc['fridge stays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states[1:]:\n",
    "    print('full ',len(noun_sentence_df_T_ri_filtered_full))\n",
    "    tic = time.time()\n",
    "    print(state)\n",
    "    noun_file = 'new_avm_df__full_data_state_'+state+'_noun_sentences.fea'\n",
    "    noun_sentence_df_T_ri_filtered = pd.read_feather(noun_file)\n",
    "    \n",
    "    noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered[noun_sentence_df_T_ri_filtered['count'] >\\\n",
    "                                                                    math.ceil(noun_sentence_df_T_ri_filtered['count'].max()*0.005)]\n",
    "\n",
    "    print(len(noun_sentence_df_T_ri_filtered))\n",
    "    \n",
    "    if len(noun_sentence_df_T_ri_filtered)>=1:\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence', 'count',\n",
    "                                                                              'AVM_Error_Indicators']]\n",
    "\n",
    "        column = 'AVM_Error_Indicators'\n",
    "        nelements = (len(noun_sentence_df_T_ri_filtered[column].values[0]))\n",
    "        list_tmp = [str(i) for i in range(nelements)] \n",
    "        column_names = [column + '_' + str(i) for i in list_tmp] \n",
    "        #print(column_names)\n",
    "        split_df = pd.DataFrame(noun_sentence_df_T_ri_filtered[column].tolist(), columns=column_names, \n",
    "                                index=noun_sentence_df_T_ri_filtered.index)\n",
    "        noun_sentence_df_T_ri_filtered = pd.concat([noun_sentence_df_T_ri_filtered, split_df], axis=1)\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.drop(columns = [column])\n",
    "\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence','count',\n",
    "                                                                               'AVM_Error_Indicators_0','AVM_Error_Indicators_1']]\n",
    "        noun_sentence_df_T_ri_filtered = df_lambda_replace_characters(noun_sentence_df_T_ri_filtered,  'noun_sentence', \n",
    "                                                           'noun_sentence_corrections', replacement_dictionary_2)\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence_corrections','count',\n",
    "                                                                               'AVM_Error_Indicators_0','AVM_Error_Indicators_1']]\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.add_suffix('_'+state)\n",
    "\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.copy(deep=True).\\\n",
    "                                    rename(columns={'noun_sentence_corrections_'+state:'noun_sentence'})\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.drop_duplicates(subset='noun_sentence')\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.set_index('noun_sentence')\n",
    "\n",
    "        print('filtered ', len(noun_sentence_df_T_ri_filtered))\n",
    "        print('added ', len(noun_sentence_df_T_ri_filtered)+len(noun_sentence_df_T_ri_filtered_full))\n",
    "        noun_sentence_df_T_ri_filtered_full = noun_sentence_df_T_ri_filtered_full.merge( noun_sentence_df_T_ri_filtered,\n",
    "                                                                                    how='outer',left_index=True, right_index=True)\n",
    "    \n",
    "        \n",
    "    print('full ',len(noun_sentence_df_T_ri_filtered_full))\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(toc - tic)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noun_sentence_df_T_ri_filtered_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noun_sentence_df_T_ri_filtered_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'noun_phrases_by_state_new_avm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.to_csv(file+'.csv')\n",
    "noun_sentence_df_T_ri_filtered_full.reset_index().to_feather(file+'.fea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full_embeddings = noun_sentence_df_T_ri_filtered_full.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'noun_sentence'\n",
    "tic = time.time()\n",
    "new_column = column + '_embeddings'\n",
    "noun_sentence_df_T_ri_filtered_full_embeddings[new_column] = noun_sentence_df_T_ri_filtered_full_embeddings[column].apply(lambda x: embed([x]).numpy()[0]) \n",
    "toc = time.time()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'noun_sentence_embeddings'\n",
    "nelements = (len(noun_sentence_df_T_ri_filtered_full_embeddings[column].values[0]))\n",
    "list_tmp = [str(i) for i in range(nelements)] \n",
    "column_names = [column + '_' + str(i) for i in list_tmp] \n",
    "print(column_names)\n",
    "split_df = pd.DataFrame(noun_sentence_df_T_ri_filtered_full_embeddings[column].tolist(), columns=column_names, \n",
    "                        index=noun_sentence_df_T_ri_filtered_full_embeddings.noun_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df = split_df.T.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = []\n",
    "corr_threshold = 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idd,sentence in enumerate(correlations_df.index):\n",
    "    print(idd)\n",
    "    #print(correlations_df.index[correlations_df.iloc[idd,:]>corr_threshold])\n",
    "    list_of_lists.append(list(correlations_df.index[correlations_df.iloc[idd,:]>corr_threshold]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df_groups = correlations_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "len_groups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(correlations_df_groups) > 0:\n",
    "    idd = correlations_df_groups.index[0]\n",
    "    print(idd)\n",
    "    print(len(correlations_df_groups))\n",
    "    group = list(np.array(correlations_df_groups.loc[idd][correlations_df_groups.loc[idd]>0.5].index))\n",
    "    groups.append(group)\n",
    "    len_groups.append(len(group))\n",
    "    correlations_df_groups = correlations_df_groups.drop(labels = group, axis = 1)\n",
    "    correlations_df_groups = correlations_df_groups.drop(labels = group, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups)\n",
    "np.array(len_groups).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_groups = np.array(len_groups).argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups[order_groups[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", colormap=\"Dark2\", max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[20,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idd, pos in enumerate(order_groups):\n",
    "    list_g = groups[pos]\n",
    "    print(idd+1, ' ',len(list_g))\n",
    "    wc.generate(' '.join(list_g))\n",
    "    plt.imshow(wc,interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_g in groups[0:1]:\n",
    "    print(len(list_g))\n",
    "    wc.generate(' '.join(list_g))\n",
    "    plt.imshow(wc,interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
