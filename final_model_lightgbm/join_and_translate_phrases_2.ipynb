{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_lambda_replace_characters(df, column_name, new_column, dictionary):\n",
    "    df[new_column] = df[column_name].apply(lambda x: replace_words(x, dictionary))       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(strtext, dictionary):\n",
    "    pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in dictionary.keys()) + r')(?!\\w)')\n",
    "    result = pattern.sub(lambda x: dictionary[x.group()], strtext)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../replacement_dictionary_ordered.json\") as json_file:\n",
    "    replacement_dictionary_2 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['TX', 'KS', 'AZ', 'TN', 'FL', 'GA', 'OK', 'UT', 'ME', 'CA', 'MS',\n",
    "       'WA', 'OH', 'MA', 'AL', 'NC', 'CO', 'MO', 'KY', 'NE', 'NY',\n",
    "       'VA', 'MD', 'HI', 'WV', 'MN', 'AK', 'IN', 'PA', 'SC', 'LA', 'NJ',\n",
    "       'RI', 'WY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = states[0]\n",
    "noun_file = 'new_avm_df__full_data_state_'+state+'_noun_sentences.fea'\n",
    "noun_sentence_df_T_ri_filtered = pd.read_feather(noun_file)\n",
    "\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered[noun_sentence_df_T_ri_filtered['count'] >\\\n",
    "                                                                noun_sentence_df_T_ri_filtered['count'].max()*0.005]\n",
    "\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence', 'count',\n",
    "                                                                      'AVM_Error_Indicators']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noun_sentence_df_T_ri_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'AVM_Error_Indicators'\n",
    "nelements = (len(noun_sentence_df_T_ri_filtered[column].values[0]))\n",
    "list_tmp = [str(i) for i in range(nelements)] \n",
    "column_names = [column + '_' + str(i) for i in list_tmp] \n",
    "print(column_names)\n",
    "split_df = pd.DataFrame(noun_sentence_df_T_ri_filtered[column].tolist(), columns=column_names, \n",
    "                        index=noun_sentence_df_T_ri_filtered.index)\n",
    "noun_sentence_df_T_ri_filtered = pd.concat([noun_sentence_df_T_ri_filtered, split_df], axis=1)\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.drop(columns = [column])\n",
    "\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence','count',\n",
    "                                                                       'AVM_Error_Indicators_0','AVM_Error_Indicators_1']]\n",
    "noun_sentence_df_T_ri_filtered = df_lambda_replace_characters(noun_sentence_df_T_ri_filtered,  'noun_sentence', \n",
    "                                                   'noun_sentence_corrections', replacement_dictionary_2)\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence_corrections','count',\n",
    "                                                                       'AVM_Error_Indicators_0','AVM_Error_Indicators_1']]\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.sort_values(by=['noun_sentence_corrections'])\n",
    "\n",
    "noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.add_suffix('_'+state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered[noun_sentence_df_T_ri_filtered['noun_sentence_corrections_TX']=='stainless steel appliances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full = noun_sentence_df_T_ri_filtered.copy(deep=True).\\\n",
    "                                rename(columns={'noun_sentence_corrections_'+state:'noun_sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated = np.unique(np.array(list(noun_sentence_df_T_ri_filtered_full[noun_sentence_df_T_ri_filtered_full['noun_sentence'].\\\n",
    "                                         duplicated()]['noun_sentence'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rrr in repeated:\n",
    "    #print(rrr)\n",
    "    tmp = noun_sentence_df_T_ri_filtered_full[noun_sentence_df_T_ri_filtered_full['noun_sentence']==rrr]\n",
    "    multi_mean  = ((tmp['count_'+state]*tmp['AVM_Error_Indicators_0_'+state]).sum())/tmp['count_'+state].sum()\n",
    "    first_part_num = ((tmp['count_'+state]*tmp['AVM_Error_Indicators_1_'+state]).sum())\n",
    "    second_part_num = ((tmp['count_'+state]*(tmp['AVM_Error_Indicators_0_'+state]-multi_mean)**2.).sum())\n",
    "    multi_variance = (first_part_num + second_part_num) / tmp['count_'+state].sum()\n",
    "    noun_sentence_df_T_ri_filtered_full['count_'+state]\\\n",
    "            [noun_sentence_df_T_ri_filtered_full['noun_sentence']==rrr] = tmp['count_'+state].sum()\n",
    "    noun_sentence_df_T_ri_filtered_full['AVM_Error_Indicators_0_'+state]\\\n",
    "            [noun_sentence_df_T_ri_filtered_full['noun_sentence']==rrr] = multi_mean\n",
    "    noun_sentence_df_T_ri_filtered_full['AVM_Error_Indicators_1_'+state]\\\n",
    "            [noun_sentence_df_T_ri_filtered_full['noun_sentence']==rrr] = multi_variance\n",
    "    \n",
    "    #print(tmp)\n",
    "    #print('MM',multi_mean)\n",
    "    #print('MV',multi_variance)\n",
    "    #print('count',tmp['count_'+state].sum())\n",
    "    #print(noun_sentence_df_T_ri_filtered_full[noun_sentence_df_T_ri_filtered_full['noun_sentence']==rrr])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full = noun_sentence_df_T_ri_filtered_full.drop_duplicates(subset='noun_sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full = noun_sentence_df_T_ri_filtered_full.set_index('noun_sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.sort_values('count_TX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.loc['fridge stays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for li in noun_sentence_df_T_ri_filtered_full.index[2900:]:\n",
    "    print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states[1:]:\n",
    "    print('full ',len(noun_sentence_df_T_ri_filtered_full))\n",
    "    tic = time.time()\n",
    "    print(state)\n",
    "    noun_file = 'new_avm_df__full_data_state_'+state+'_noun_sentences.fea'\n",
    "    noun_sentence_df_T_ri_filtered = pd.read_feather(noun_file)\n",
    "    \n",
    "    noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered[noun_sentence_df_T_ri_filtered['count'] >\\\n",
    "                                                                    math.ceil(noun_sentence_df_T_ri_filtered['count'].max()*0.005)]\n",
    "\n",
    "    print(len(noun_sentence_df_T_ri_filtered))\n",
    "    \n",
    "    if len(noun_sentence_df_T_ri_filtered)>=1:\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence', 'count',\n",
    "                                                                              'AVM_Error_Indicators']]\n",
    "\n",
    "        column = 'AVM_Error_Indicators'\n",
    "        nelements = (len(noun_sentence_df_T_ri_filtered[column].values[0]))\n",
    "        list_tmp = [str(i) for i in range(nelements)] \n",
    "        column_names = [column + '_' + str(i) for i in list_tmp] \n",
    "        #print(column_names)\n",
    "        split_df = pd.DataFrame(noun_sentence_df_T_ri_filtered[column].tolist(), columns=column_names, \n",
    "                                index=noun_sentence_df_T_ri_filtered.index)\n",
    "        noun_sentence_df_T_ri_filtered = pd.concat([noun_sentence_df_T_ri_filtered, split_df], axis=1)\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.drop(columns = [column])\n",
    "\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence','count',\n",
    "                                                                               'AVM_Error_Indicators_0','AVM_Error_Indicators_1']]\n",
    "        noun_sentence_df_T_ri_filtered = df_lambda_replace_characters(noun_sentence_df_T_ri_filtered,  'noun_sentence', \n",
    "                                                           'noun_sentence_corrections', replacement_dictionary_2)\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.loc[:,['noun_sentence_corrections','count',\n",
    "                                                                               'AVM_Error_Indicators_0','AVM_Error_Indicators_1']]\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.sort_values(by=['noun_sentence_corrections'])\n",
    "        \n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.add_suffix('_'+state)\n",
    "        \n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.\\\n",
    "                                    rename(columns={'noun_sentence_corrections_'+state:'noun_sentence'})\n",
    "        \n",
    "        repeated = np.unique(np.array(list(noun_sentence_df_T_ri_filtered\\\n",
    "                                           [noun_sentence_df_T_ri_filtered['noun_sentence'].\\\n",
    "                                           duplicated()]['noun_sentence'].values)))\n",
    "        \n",
    "        for rrr in repeated:\n",
    "            #print(rrr)\n",
    "            tmp = noun_sentence_df_T_ri_filtered[noun_sentence_df_T_ri_filtered['noun_sentence']==rrr]\n",
    "            multi_mean  = ((tmp['count_'+state]*tmp['AVM_Error_Indicators_0_'+state]).sum())/tmp['count_'+state].sum()\n",
    "            first_part_num = ((tmp['count_'+state]*tmp['AVM_Error_Indicators_1_'+state]).sum())\n",
    "            second_part_num = ((tmp['count_'+state]*(tmp['AVM_Error_Indicators_0_'+state]-multi_mean)**2.).sum())\n",
    "            multi_variance = (first_part_num + second_part_num) / tmp['count_'+state].sum()\n",
    "            noun_sentence_df_T_ri_filtered['count_'+state]\\\n",
    "                    [noun_sentence_df_T_ri_filtered['noun_sentence']==rrr] = tmp['count_'+state].sum()\n",
    "            noun_sentence_df_T_ri_filtered['AVM_Error_Indicators_0_'+state]\\\n",
    "                    [noun_sentence_df_T_ri_filtered['noun_sentence']==rrr] = multi_mean\n",
    "            noun_sentence_df_T_ri_filtered['AVM_Error_Indicators_1_'+state]\\\n",
    "                    [noun_sentence_df_T_ri_filtered['noun_sentence']==rrr] = multi_variance\n",
    "\n",
    "\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.drop_duplicates(subset='noun_sentence')\n",
    "        noun_sentence_df_T_ri_filtered = noun_sentence_df_T_ri_filtered.set_index('noun_sentence')\n",
    "\n",
    "        print('filtered ', len(noun_sentence_df_T_ri_filtered))\n",
    "        print('added ', len(noun_sentence_df_T_ri_filtered)+len(noun_sentence_df_T_ri_filtered_full))\n",
    "        noun_sentence_df_T_ri_filtered_full = noun_sentence_df_T_ri_filtered_full.merge( noun_sentence_df_T_ri_filtered,\n",
    "                                                                                    how='outer',left_index=True, right_index=True)\n",
    "    \n",
    "        \n",
    "    print('full ',len(noun_sentence_df_T_ri_filtered_full))\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(toc - tic)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noun_sentence_df_T_ri_filtered_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noun_sentence_df_T_ri_filtered_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'noun_phrases_by_state_new_avm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.to_csv(file+'.csv')\n",
    "noun_sentence_df_T_ri_filtered_full.reset_index().to_feather(file+'.fea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_prhases_new_data = pd.DataFrame(columns = ['noun_sentence', 'state', 'count', 'mean', 'variance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stateg in states:\n",
    "    #print(noun_sentence_df_T_ri_filtered_full.columns)\n",
    "    print(stateg)\n",
    "    res = [i for i in list(noun_sentence_df_T_ri_filtered_full.columns) if stateg in i]\n",
    "    print(res)\n",
    "    if len(res)>0:\n",
    "        tmp = noun_sentence_df_T_ri_filtered_full.loc[:,['count_'+stateg, 'AVM_Error_Indicators_0_'+stateg, 'AVM_Error_Indicators_1_'+stateg]]\n",
    "        tmp = tmp.rename(columns={'count_'+stateg:'count',\n",
    "                                  'AVM_Error_Indicators_0_'+stateg:'mean',\n",
    "                                  'AVM_Error_Indicators_1_'+stateg:'variance'})\n",
    "        tmp['state'] = stateg\n",
    "        tmp = tmp.reset_index()\n",
    "        noun_prhases_new_data = noun_prhases_new_data.append(tmp,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_prhases_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_prhases_new_data.to_csv('noun_phrases_new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full.loc[:,['count_'+stateg, 'AVM_Error_Indicators_0_'+stateg, 'AVM_Error_Indicators_1_'+stateg]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateg='TX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['count_'+stateg, 'AVM_Error_Indicators_0_'+stateg, 'AVM_Error_Indicators_1_'+stateg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full_embeddings = noun_sentence_df_T_ri_filtered_full.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'noun_sentence'\n",
    "tic = time.time()\n",
    "new_column = column + '_embeddings'\n",
    "noun_sentence_df_T_ri_filtered_full_embeddings[new_column] = noun_sentence_df_T_ri_filtered_full_embeddings[column].apply(lambda x: embed([x]).numpy()[0]) \n",
    "toc = time.time()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered_full_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'noun_sentence_embeddings'\n",
    "nelements = (len(noun_sentence_df_T_ri_filtered_full_embeddings[column].values[0]))\n",
    "list_tmp = [str(i) for i in range(nelements)] \n",
    "column_names = [column + '_' + str(i) for i in list_tmp] \n",
    "print(column_names)\n",
    "split_df = pd.DataFrame(noun_sentence_df_T_ri_filtered_full_embeddings[column].tolist(), columns=column_names, \n",
    "                        index=noun_sentence_df_T_ri_filtered_full_embeddings.noun_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df = split_df.T.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = []\n",
    "corr_threshold = 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idd,sentence in enumerate(correlations_df.index):\n",
    "    print(idd)\n",
    "    #print(correlations_df.index[correlations_df.iloc[idd,:]>corr_threshold])\n",
    "    list_of_lists.append(list(correlations_df.index[correlations_df.iloc[idd,:]>corr_threshold]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_prhase_map = pd.DataFrame(columns = ['noun_phrase', 'maps_to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countc = 0\n",
    "for idd,list_id in enumerate(list_of_lists):\n",
    "    flag = len(noun_prhase_map[noun_prhase_map['noun_phrase']==list_id[0]])\n",
    "    print(idd, flag, countc)\n",
    "    print(list_id)\n",
    "    if flag==0:\n",
    "        countc += len(list_id)\n",
    "        for element in list_id:\n",
    "            noun_prhase_map = noun_prhase_map.append({'noun_phrase' : element, 'maps_to' : list_id[0]},  ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_prhase_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noun_prhase_map[noun_prhase_map['noun_phrase']==list_id[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noun_prhase_map[noun_prhase_map['noun_phrase']=='casa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_prhase_map.to_csv('noun_phrase_map.csv')\n",
    "noun_prhase_map.reset_index().to_feather('noun_phrase_map.fea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df_groups = correlations_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "len_groups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(correlations_df_groups) > 0:\n",
    "    idd = correlations_df_groups.index[0]\n",
    "    print(idd)\n",
    "    print(len(correlations_df_groups))\n",
    "    group = list(np.array(correlations_df_groups.loc[idd][correlations_df_groups.loc[idd]>0.4].index))\n",
    "    groups.append(group)\n",
    "    len_groups.append(len(group))\n",
    "    correlations_df_groups = correlations_df_groups.drop(labels = group, axis = 1)\n",
    "    correlations_df_groups = correlations_df_groups.drop(labels = group, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups)\n",
    "np.array(len_groups).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_groups = np.array(len_groups).argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups[order_groups[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_prhase_groups = pd.DataFrame(columns = ['group','noun_phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idd, pos in enumerate(order_groups):\n",
    "    list_g = groups[pos]\n",
    "    print(idd+1, ' ',len(list_g))\n",
    "    for element in list_g:\n",
    "        noun_prhase_groups = noun_prhase_groups.append({'group' : idd+1, 'noun_phrase' : element},  ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_prhase_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_prhase_groups.to_csv('noun_phrase_groups.csv')\n",
    "noun_prhase_groups.reset_index().to_feather('noun_phrase_groups.fea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", colormap=\"Dark2\", max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[20,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idd, pos in enumerate(order_groups):\n",
    "    list_g = groups[pos]\n",
    "    print(idd+1, ' ',len(list_g))\n",
    "    wc.generate(' '.join(list_g))\n",
    "    plt.imshow(wc,interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_sentence_df_T_ri_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_g in groups[0:1]:\n",
    "    print(len(list_g))\n",
    "    wc.generate(' '.join(list_g))\n",
    "    plt.imshow(wc,interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
