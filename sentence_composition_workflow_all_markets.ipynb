{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/cdsw/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_files = glob.glob(\"comments/raw_comments/*_filtered.fea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_types(strtext, TAGs):\n",
    "\n",
    "    textblob_temp = TextBlob(strtext)\n",
    "    #polarity      = textblob_temp.sentiment.polarity\n",
    "    #subjectivity  = textblob_temp.sentiment.subjectivity\n",
    "    count= Counter([j for i,j in pos_tag(word_tokenize(strtext))])\n",
    "    \n",
    "    total_words = len(textblob_temp.words)\n",
    "    #print(total_words)\n",
    "    \n",
    "    count_list = []\n",
    "    for listt in TAGs:\n",
    "        class_counter = 0\n",
    "        for item in listt:\n",
    "            class_counter += count[item]\n",
    "        if (total_words!=0):\n",
    "            count_list.append(class_counter/total_words*100.)\n",
    "        else:\n",
    "            count_list.append(0)\n",
    "   \n",
    "    count_list.append(total_words)\n",
    "    count_list.append(len(textblob_temp.np_counts))\n",
    "    \n",
    "    #textblob_temp = TextBlob(strtext, analyzer = NaiveBayesAnalyzer())\n",
    "    #classification= textblob_temp.sentiment.classification\n",
    "    #positive      = textblob_temp.sentiment.p_pos\n",
    "    #negative      = textblob_temp.sentiment.p_neg\n",
    "\n",
    "    #print(polarity,subjectivity)\n",
    "    \n",
    "    #count_list.append(polarity)\n",
    "    #count_list.append(subjectivity)\n",
    "    \n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'clean_publicremarks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_types = ['Nouns', 'Verbs', 'Adjectives', 'Adverbs', 'Prepositions', 'Modal', 'PersonalP']\n",
    "\n",
    "TAGs = [['NN','NNS','NNP','NNPS'],['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'], ['JJ', 'JJR', 'JJS'], \n",
    "       ['RB', 'RBR', 'RBS'], ['IN'], ['MD'], ['PRP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments/raw_comments/agent_comments_AL_filtered.fea\n",
      "219915\n",
      "List ready\n",
      "2671.8857333660126\n",
      "comments/raw_comments/agent_comments_AR_filtered.fea\n",
      "100806\n",
      "List ready\n",
      "761.5070123672485\n",
      "comments/raw_comments/agent_comments_AZ_filtered.fea\n",
      "405421\n",
      "List ready\n",
      "2599.785404443741\n",
      "comments/raw_comments/agent_comments_CA_filtered.fea\n",
      "2103405\n",
      "List ready\n",
      "23140.113981962204\n",
      "comments/raw_comments/agent_comments_CO_filtered.fea\n",
      "588449\n",
      "List ready\n",
      "7244.933393478394\n",
      "comments/raw_comments/agent_comments_CT_filtered.fea\n",
      "323150\n",
      "List ready\n",
      "3709.1479654312134\n",
      "comments/raw_comments/agent_comments_DC_filtered.fea\n",
      "5970\n",
      "List ready\n",
      "53.60975527763367\n",
      "comments/raw_comments/agent_comments_DE_filtered.fea\n",
      "28154\n",
      "List ready\n",
      "379.8615505695343\n",
      "comments/raw_comments/agent_comments_FL_filtered.fea\n",
      "1904538\n",
      "List ready\n",
      "26554.074403762817\n",
      "comments/raw_comments/agent_comments_GA_filtered.fea\n",
      "735424\n",
      "List ready\n",
      "7705.266858577728\n",
      "comments/raw_comments/agent_comments_HI_filtered.fea\n",
      "27440\n",
      "List ready\n",
      "295.9163239002228\n",
      "comments/raw_comments/agent_comments_IA_filtered.fea\n",
      "9900\n",
      "List ready\n",
      "79.69210290908813\n",
      "comments/raw_comments/agent_comments_ID_filtered.fea\n",
      "169742\n",
      "List ready\n",
      "1590.8274977207184\n",
      "comments/raw_comments/agent_comments_IL_filtered.fea\n",
      "839221\n",
      "List ready\n",
      "9921.283560752869\n",
      "comments/raw_comments/agent_comments_IN_filtered.fea\n",
      "151574\n",
      "List ready\n",
      "1788.1967902183533\n",
      "comments/raw_comments/agent_comments_KS_filtered.fea\n",
      "147183\n",
      "List ready\n",
      "1699.051293373108\n",
      "comments/raw_comments/agent_comments_KY_filtered.fea\n",
      "242567\n",
      "List ready\n",
      "2930.6122715473175\n",
      "comments/raw_comments/agent_comments_LA_filtered.fea\n",
      "250540\n",
      "List ready\n",
      "2527.0489230155945\n",
      "comments/raw_comments/agent_comments_MA_filtered.fea\n",
      "559949\n",
      "List ready\n",
      "7323.210572719574\n",
      "comments/raw_comments/agent_comments_MD_filtered.fea\n",
      "193090\n",
      "List ready\n",
      "2027.8255906105042\n",
      "comments/raw_comments/agent_comments_ME_filtered.fea\n",
      "44551\n",
      "List ready\n",
      "404.6676926612854\n",
      "comments/raw_comments/agent_comments_MI_filtered.fea\n",
      "596752\n",
      "List ready\n",
      "7029.3040771484375\n",
      "comments/raw_comments/agent_comments_MN_filtered.fea\n",
      "528185\n"
     ]
    }
   ],
   "source": [
    "for file in filtered_files:\n",
    "    print(file)\n",
    "    df_sales = pd.read_feather(file)\n",
    "    print(len(df_sales))\n",
    "    \n",
    "    tic = time.time()\n",
    "    list_result = df_sales[column].apply(lambda x: count_word_types(x, TAGs))\n",
    "    print('List ready')\n",
    "  \n",
    "    df_sales_use = pd.DataFrame({'sentence_composition':list_result.values, 'listingid': df_sales['listingid'].values},\n",
    "                 index=df_sales.index) \n",
    "    \n",
    "    nelements = (len(df_sales_use['sentence_composition'][0]))\n",
    "    list_tmp = [str(i) for i in range(nelements)] \n",
    "    column_names = ['sentence_composition' + '_' + str(i) for i in list_tmp] \n",
    "    split_df = pd.DataFrame(df_sales_use['sentence_composition'].tolist(), columns=column_names)\n",
    "    df_sales_use = pd.concat([df_sales_use, split_df], axis=1)\n",
    "    df_sales_use = df_sales_use.drop(columns = ['sentence_composition'])\n",
    "    \n",
    "    sentence_file = file.split('.')[0] + '_sentence_composition.fea'\n",
    "    toc = time.time()\n",
    "    print(toc - tic)\n",
    "    df_sales_use.to_feather(sentence_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO SENTIMENT ON COMBINED NOUN SENTENCES -  combine this with ngram percentile..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words 99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'full brick home': 1,\n",
       "             'corner lot': 1,\n",
       "             'yard privacy': 1,\n",
       "             'large great room fireplace gas logs': 1,\n",
       "             'beautiful kitchen': 1,\n",
       "             'granite countertops': 1,\n",
       "             'island walk laundry room': 1,\n",
       "             'spacious master bedroom': 1,\n",
       "             'large master bath': 1,\n",
       "             'tub relaxation': 1,\n",
       "             'separate shower walk closet': 1,\n",
       "             'main level guests': 1,\n",
       "             'bonus room': 1,\n",
       "             'full bath': 1,\n",
       "             'garage doors': 1,\n",
       "             'new oak stair case': 1,\n",
       "             'new den hardwood plank': 1,\n",
       "             'great room brand': 1,\n",
       "             'new pagola': 1,\n",
       "             'fantastic home': 1,\n",
       "             'great neighborhood': 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfg = TextBlob(df_sales['clean_publicremarks'].values[500])\n",
    "print('# words', len(gfg.words))\n",
    "      \n",
    "# using TextBlob.word_counts() method\n",
    "gfg = gfg.np_counts\n",
    "gfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('opportunity', 'NN'), ('knocking', 'VBG'), ('answer', 'VB'), ('?', '.'), ('?', '.'), ('?', '.'), ('renovated', 'VBD'), ('home', 'NN'), ('sought', 'VBN'), ('family', 'NN'), ('oriented', 'VBN'), ('sturbridge', 'NN'), ('community', 'NN'), ('stones', 'NNS'), ('throw', 'VBP'), ('pristine', 'JJ'), ('shores', 'NNS'), ('west', 'JJS'), ('point', 'NN'), ('lake', 'NN'), ('&', 'CC'), ('sunny', 'JJ'), ('point', 'NN'), ('access', 'NN'), ('&', 'CC'), ('picnic', 'JJ'), ('area', 'NN'), ('&', 'CC'), ('minutes', 'NNS'), ('everything', 'NN'), ('lagrange', 'NN'), ('offer', 'NN'), ('!', '.'), ('spacious', 'JJ'), ('floor', 'NN'), ('plan', 'NN'), ('loaded', 'VBD'), ('amenities', 'NNS'), (',', ','), ('updated', 'VBD'), ('chefs', 'NNS'), ('kitchen', 'VB'), ('w', 'JJ'), ('new', 'JJ'), ('cabinetry', 'NN'), (',', ','), ('stone', 'NN'), ('tops', 'NN'), ('&', 'CC'), ('stainless', 'NN'), ('appliance', 'NN'), ('package', 'NN'), ('&', 'CC'), ('adjoining', 'VBG'), ('formal', 'JJ'), ('dinning', 'NN'), ('room', 'NN'), ('perfect', 'VBP'), ('large', 'JJ'), ('family', 'NN'), ('celebrations', 'NNS'), (',', ','), ('beautiful', 'JJ'), ('newly', 'RB'), ('installed', 'VBN'), ('luxury', 'NN'), ('vinyl', 'NN'), ('plank', 'NN'), ('flooring', 'VBG'), ('common', 'JJ'), ('areas', 'NNS'), (',', ','), ('updated', 'VBD'), ('hvac', 'NN'), ('&', 'CC'), ('new', 'JJ'), ('windows', 'NNS'), ('throughout', 'IN'), ('home', 'NN'), (',', ','), ('generous', 'JJ'), ('master', 'NN'), ('suite', 'NN'), ('w', 'NN'), ('updated', 'VBN'), ('bath', 'NN'), ('&', 'CC'), ('abundance', 'NN'), ('closet', 'NN'), ('space', 'NN'), (',', ','), ('gigantic', 'JJ'), ('bonus', 'NN'), ('room', 'NN'), ('garage', 'NN'), ('perfect', 'JJ'), ('playroom/office/man', 'NN'), ('cave', 'NN'), ('must', 'MD'), ('see', 'VB'), ('hill', 'JJ'), ('crest', 'JJS'), ('school', 'NN'), ('zone', 'NN'), ('!', '.'), ('!', '.'), ('!', '.'), ('!', '.')]\n",
      "Counter({'NN': 43, 'JJ': 15, '.': 8, 'NNS': 8, 'CC': 7, ',': 6, 'VBD': 4, 'VBN': 4, 'VBG': 3, 'VB': 3, 'VBP': 2, 'JJS': 2, 'RB': 1, 'IN': 1, 'MD': 1})\n",
      "['NN', 'VBG', 'VB', '.', 'VBD', 'VBN', 'NNS', 'VBP', 'JJ', 'JJS', 'CC', ',', 'RB', 'IN', 'MD']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "sent = df_sales['clean_publicremarks'].values[500]\n",
    "text= pos_tag(word_tokenize(sent))\n",
    "print(text)\n",
    "\n",
    "from collections import Counter\n",
    "count= Counter([j for i,j in pos_tag(word_tokenize(sent))])\n",
    "print(count)\n",
    "print(list(count))\n",
    "count['PDT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "#text.similar('woman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
